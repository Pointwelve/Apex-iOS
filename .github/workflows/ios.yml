name: iOS Build & Test

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'Apex/**'
      - '.github/workflows/ios.yml'
      - '.swiftlint.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'Apex/**'
      - '.github/workflows/ios.yml'
      - '.swiftlint.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    runs-on: macos-15
    if: github.event_name == 'pull_request' || contains(github.event.head_commit.message, '[lint]')
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Cache SwiftLint
      uses: actions/cache@v4
      with:
        path: |
          /opt/homebrew/bin/swiftlint
          /usr/local/bin/swiftlint
        key: swiftlint-${{ runner.os }}-v1
        restore-keys: swiftlint-${{ runner.os }}-
    
    - name: Install SwiftLint
      run: |
        if ! command -v swiftlint &> /dev/null; then
          echo "Installing SwiftLint..."
          brew install swiftlint
        else
          echo "SwiftLint already installed: $(which swiftlint)"
          swiftlint version
        fi
    
    - name: Run SwiftLint
      run: swiftlint --strict --reporter github-actions-logging
      
    - name: Upload SwiftLint results
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: swiftlint-results
        path: swiftlint-results.json
        retention-days: 7

  test-impact-analysis:
    runs-on: macos-15
    outputs:
      should_run_tests: ${{ steps.analyze.outputs.should_run_tests }}
      test_targets: ${{ steps.analyze.outputs.test_targets }}
      test_strategy: ${{ steps.analyze.outputs.test_strategy }}
      changed_files_count: ${{ steps.analyze.outputs.changed_files_count }}
    timeout-minutes: 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup Python environment
      run: |
        # Create virtual environment to avoid externally-managed-environment error
        python3 -m venv venv
        source venv/bin/activate
        python3 -m pip install --upgrade pip
        python3 -m pip install pyyaml || echo "PyYAML installation failed, will fallback to running all tests"
        echo "VIRTUAL_ENV_CREATED=true" >> $GITHUB_ENV
    
    - name: Analyze test impact
      id: analyze
      run: |
        chmod +x scripts/test-impact-analysis.sh
        
        # Run test impact analysis
        result=$(./scripts/test-impact-analysis.sh analyze --verbose)
        echo "Analysis result: $result"
        
        # Parse the result and set outputs
        if [[ "$result" == "NO_CHANGES" ]]; then
          echo "should_run_tests=false" >> $GITHUB_OUTPUT
          echo "test_strategy=skip" >> $GITHUB_OUTPUT
          echo "changed_files_count=0" >> $GITHUB_OUTPUT
        elif [[ "$result" == "SKIP_TESTS" ]]; then
          echo "should_run_tests=false" >> $GITHUB_OUTPUT
          echo "test_strategy=skip-docs-only" >> $GITHUB_OUTPUT
          echo "changed_files_count=1" >> $GITHUB_OUTPUT
        elif [[ "$result" == "RUN_ALL_TESTS" ]]; then
          echo "should_run_tests=true" >> $GITHUB_OUTPUT
          echo "test_targets=ApexTests,ApexUITests" >> $GITHUB_OUTPUT
          echo "test_strategy=full" >> $GITHUB_OUTPUT
          # Count changed files for reporting
          changed_count=$(git diff --name-only HEAD~1 HEAD | wc -l)
          echo "changed_files_count=$changed_count" >> $GITHUB_OUTPUT
        elif [[ "$result" == TESTS=* ]]; then
          echo "should_run_tests=true" >> $GITHUB_OUTPUT
          targets=$(echo "$result" | sed 's/TESTS=//')
          echo "test_targets=$targets" >> $GITHUB_OUTPUT
          echo "test_strategy=targeted" >> $GITHUB_OUTPUT
          # Count changed files for reporting
          changed_count=$(git diff --name-only HEAD~1 HEAD | wc -l)
          echo "changed_files_count=$changed_count" >> $GITHUB_OUTPUT
        else
          # Fallback to running all tests
          echo "should_run_tests=true" >> $GITHUB_OUTPUT
          echo "test_targets=ApexTests,ApexUITests" >> $GITHUB_OUTPUT
          echo "test_strategy=fallback" >> $GITHUB_OUTPUT
          echo "changed_files_count=unknown" >> $GITHUB_OUTPUT
        fi
        
        # Generate summary for workflow
        echo "## ðŸ§ª Test Impact Analysis" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        case "$(echo "$result" | head -1)" in
          "NO_CHANGES")
            echo "**Result:** No changes detected - tests will be skipped" >> $GITHUB_STEP_SUMMARY
            ;;
          "SKIP_TESTS")
            echo "**Result:** Only documentation files changed - tests will be skipped" >> $GITHUB_STEP_SUMMARY
            ;;
          "RUN_ALL_TESTS")
            echo "**Result:** Significant changes detected - running all tests" >> $GITHUB_STEP_SUMMARY
            echo "**Targets:** Unit tests + UI tests" >> $GITHUB_STEP_SUMMARY
            ;;
          TESTS=*)
            targets=$(echo "$result" | head -1 | sed 's/TESTS=//')
            echo "**Result:** Targeted testing based on file analysis" >> $GITHUB_STEP_SUMMARY
            echo "**Targets:** $targets" >> $GITHUB_STEP_SUMMARY
            ;;
          *)
            echo "**Result:** Fallback to running all tests" >> $GITHUB_STEP_SUMMARY
            ;;
        esac

  build-and-test:
    runs-on: macos-15
    needs: [lint, test-impact-analysis]
    if: |
      always() && 
      (needs.lint.result == 'success' || needs.lint.result == 'skipped') &&
      needs.test-impact-analysis.outputs.should_run_tests == 'true'
    timeout-minutes: 30
    
    strategy:
      matrix:
        destination: 
          - 'platform=iOS Simulator,name=iPhone 16 Pro,OS=18.0'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Select Xcode 16.4
      run: sudo xcode-select -switch /Applications/Xcode_16.4.app/Contents/Developer
    
    - name: Show Xcode and SDK info
      run: |
        /usr/bin/xcodebuild -version
        /usr/bin/xcodebuild -showsdks
    
    - name: Cache Ruby gems
      uses: actions/cache@v4
      with:
        path: ~/.gem
        key: ${{ runner.os }}-gems-${{ hashFiles('**/Gemfile.lock') }}
        restore-keys: |
          ${{ runner.os }}-gems-
    
    - name: Install xcpretty
      run: |
        if ! command -v xcpretty &> /dev/null; then
          echo "Installing xcpretty..."
          gem install xcpretty
        else
          echo "xcpretty already installed: $(which xcpretty)"
        fi
    
    - name: Cache Xcode DerivedData
      uses: actions/cache@v4
      with:
        path: ~/Library/Developer/Xcode/DerivedData
        key: ${{ runner.os }}-deriveddata-${{ hashFiles('Apex/**/*.swift', 'Apex/**/*.plist', 'Apex/**/*.xcconfig', 'Apex/**/project.pbxproj') }}
        restore-keys: |
          ${{ runner.os }}-deriveddata-
    
    - name: Cache Swift Package Manager
      uses: actions/cache@v4
      with:
        path: |
          Apex/.build
          ~/Library/Caches/org.swift.swiftpm
          ~/.swiftpm/cache
          ~/Library/org.swift.swiftpm
        key: ${{ runner.os }}-spm-${{ hashFiles('Apex/**/Package.swift', 'Apex/**/Package.resolved') }}-v2
        restore-keys: |
          ${{ runner.os }}-spm-
          ${{ runner.os }}-spm-v2-
    
    - name: Build for testing
      run: |
        cd Apex
        
        # Use aggressive build optimization
        set -o pipefail && xcodebuild \
          -project Apex.xcodeproj \
          -scheme Apex \
          -destination '${{ matrix.destination }}' \
          -derivedDataPath ./DerivedData \
          -clonedSourcePackagesDirPath ./DerivedData/SourcePackages \
          -resultBundlePath ./BuildResults \
          -parallelizeTargets \
          -parallel-testing-enabled YES \
          -maximum-parallel-testing-workers auto \
          -disable-concurrent-destination-testing \
          BUILD_LIBRARY_FOR_DISTRIBUTION=NO \
          COMPILER_INDEX_STORE_ENABLE=NO \
          build-for-testing | xcpretty
    
    - name: Run targeted tests
      run: |
        cd Apex
        
        # Get test targets from analysis
        test_targets="${{ needs.test-impact-analysis.outputs.test_targets }}"
        test_strategy="${{ needs.test-impact-analysis.outputs.test_strategy }}"
        
        echo "ðŸŽ¯ Test Strategy: $test_strategy"
        echo "ðŸ“‹ Test Targets: $test_targets"
        
        # Generate and run test commands based on strategy
        if [[ "$test_strategy" == "full" ]] || [[ "$test_strategy" == "fallback" ]]; then
          echo "Running all tests..."
          set -o pipefail && xcodebuild \
            -project Apex.xcodeproj \
            -scheme Apex \
            -destination '${{ matrix.destination }}' \
            -derivedDataPath ./DerivedData \
            -resultBundlePath ./TestResults \
            -parallel-testing-enabled YES \
            -maximum-parallel-testing-workers auto \
            test-without-building | xcpretty
        elif [[ "$test_strategy" == "targeted" ]]; then
          echo "Running targeted tests: $test_targets"
          
          # Convert comma-separated targets to array and run each
          IFS=',' read -ra TARGETS <<< "$test_targets"
          test_failed=false
          
          for target in "${TARGETS[@]}"; do
            echo "ðŸ”¬ Running tests for target: $target"
            
            if ! set -o pipefail && xcodebuild \
              -project Apex.xcodeproj \
              -scheme Apex \
              -destination '${{ matrix.destination }}' \
              -derivedDataPath ./DerivedData \
              -resultBundlePath ./TestResults-$target \
              -parallel-testing-enabled YES \
              -maximum-parallel-testing-workers auto \
              -only-testing:$target \
              test-without-building | xcpretty; then
              test_failed=true
              echo "âŒ Tests failed for target: $target"
            else
              echo "âœ… Tests passed for target: $target"
            fi
          done
          
          # Combine test results
          mkdir -p ./TestResults
          find . -name "TestResults-*" -type d -exec cp -r {} ./TestResults/ \;
          
          if [[ "$test_failed" == "true" ]]; then
            echo "âŒ Some targeted tests failed"
            exit 1
          fi
        else
          echo "âš ï¸ Unknown test strategy: $test_strategy, running all tests as fallback"
          set -o pipefail && xcodebuild \
            -project Apex.xcodeproj \
            -scheme Apex \
            -destination '${{ matrix.destination }}' \
            -derivedDataPath ./DerivedData \
            -resultBundlePath ./TestResults \
            -parallel-testing-enabled YES \
            -maximum-parallel-testing-workers auto \
            test-without-building | xcpretty
        fi
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ needs.test-impact-analysis.outputs.test_strategy }}-${{ strategy.job-index }}
        path: |
          Apex/TestResults*
          Apex/BuildResults
        retention-days: 14
    
    - name: Parse test results
      if: always()
      run: |
        if [ -d "Apex/TestResults" ]; then
          find Apex/TestResults -name "*.xcresult" -exec xcrun xcresulttool get --format json --path {} \; > test-summary.json || true
        fi
    
    - name: Upload test summary
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-summary-${{ needs.test-impact-analysis.outputs.test_strategy }}-${{ strategy.job-index }}
        path: test-summary.json
        retention-days: 14
    
    - name: Generate test optimization report
      if: always()
      run: |
        echo "## ðŸ“Š Test Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Strategy:** ${{ needs.test-impact-analysis.outputs.test_strategy }}" >> $GITHUB_STEP_SUMMARY
        echo "**Targets:** ${{ needs.test-impact-analysis.outputs.test_targets }}" >> $GITHUB_STEP_SUMMARY
        echo "**Changed Files:** ${{ needs.test-impact-analysis.outputs.changed_files_count }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Calculate time savings (estimated)
        case "${{ needs.test-impact-analysis.outputs.test_strategy }}" in
          "targeted")
            target_count=$(echo "${{ needs.test-impact-analysis.outputs.test_targets }}" | tr ',' '\n' | wc -l)
            if [[ $target_count -eq 1 ]]; then
              echo "âš¡ **Estimated Time Savings:** ~50% (running 1 test suite instead of 2)" >> $GITHUB_STEP_SUMMARY
            else
              echo "â„¹ï¸  **Time Savings:** Minimal (running all test suites)" >> $GITHUB_STEP_SUMMARY
            fi
            ;;
          "skip")
            echo "âš¡ **Time Savings:** ~100% (tests skipped - no relevant changes)" >> $GITHUB_STEP_SUMMARY
            ;;
          "skip-docs-only")
            echo "âš¡ **Time Savings:** ~100% (tests skipped - documentation changes only)" >> $GITHUB_STEP_SUMMARY
            ;;
          "full")
            echo "â„¹ï¸  **Time Savings:** None (significant changes require full testing)" >> $GITHUB_STEP_SUMMARY
            ;;
          "fallback")
            echo "âš ï¸  **Time Savings:** None (fallback to full testing for safety)" >> $GITHUB_STEP_SUMMARY
            ;;
        esac

  test-summary:
    runs-on: ubuntu-latest
    needs: [test-impact-analysis, build-and-test]
    if: always()
    
    steps:
    - name: Generate final summary
      run: |
        echo "## ðŸ Build & Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Test impact analysis results
        if [[ "${{ needs.test-impact-analysis.outputs.should_run_tests }}" == "false" ]]; then
          echo "âœ… **Tests:** Skipped (no relevant changes)" >> $GITHUB_STEP_SUMMARY
          echo "âš¡ **Performance:** Optimized - saved ~5-10 minutes" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.build-and-test.result }}" == "success" ]]; then
          echo "âœ… **Tests:** Passed (${{ needs.test-impact-analysis.outputs.test_strategy }} strategy)" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.build-and-test.result }}" == "failure" ]]; then
          echo "âŒ **Tests:** Failed (${{ needs.test-impact-analysis.outputs.test_strategy }} strategy)" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.build-and-test.result }}" == "skipped" ]]; then
          echo "â­ï¸  **Tests:** Skipped" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Impact Analysis:**" >> $GITHUB_STEP_SUMMARY
        echo "- Changed files: ${{ needs.test-impact-analysis.outputs.changed_files_count }}" >> $GITHUB_STEP_SUMMARY
        echo "- Test strategy: ${{ needs.test-impact-analysis.outputs.test_strategy }}" >> $GITHUB_STEP_SUMMARY
        
        if [[ "${{ needs.test-impact-analysis.outputs.test_targets }}" != "" ]]; then
          echo "- Test targets: ${{ needs.test-impact-analysis.outputs.test_targets }}" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**ðŸ“ Note:** This pipeline only builds and tests. For releases, use the 'Release to TestFlight & App Store' workflow." >> $GITHUB_STEP_SUMMARY